{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import ast\n",
    "import tiktoken\n",
    "import requests\n",
    "import re\n",
    "import os,glob\n",
    "import time\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_from_pdf(pdf_files,filter_ref = False, combine=False):\n",
    "    data = []\n",
    "    pdf_content_dict = {}\n",
    "    for pdf in pdf_files:\n",
    "        all_pdf_txt = ''\n",
    "        print(pdf)\n",
    "        with open(pdf, 'rb') as pdf_content:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                page_text = page.extract_text()\n",
    "                all_pdf_txt += page_text\n",
    "            all_pdf_txt = re.split('\\n',all_pdf_txt)\n",
    "            all_pdf_txt = [txt1 for txt in all_pdf_txt for txt1 in re.split('\\n \\n',txt)]\n",
    "            for i, para_part in enumerate(all_pdf_txt):\n",
    "                data.append({\n",
    "                    'file name': pdf,\n",
    "                    'paragraph number': i+1,\n",
    "                    'content': para_part,\n",
    "                    'tokens': count_tokens(para_part)\n",
    "                })\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ]
    }
   ],
   "source": [
    "folder_name = r'E:\\liuming\\cage-GPT\\papers\\*\\*.pdf'\n",
    "#folder_name = r'E:\\liuming\\cage-GPT\\additional papers\\finished\\*.pdf'\n",
    "all_papers = glob.glob(folder_name)\n",
    "df = get_txt_from_pdf(all_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paper_list = list(pdf_content_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = pdf_content_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "all_groups = []\n",
    "data = []\n",
    "for i in range(len(df)):\n",
    "    pdf = df['file name'][i]\n",
    "    content_split = df['content'][i].split()\n",
    "    if content_split != []:\n",
    "        content_end = content_split[-1][-1]\n",
    "        if content_end == '.' or content_end == '*':\n",
    "            all_groups.append(range(start_index,i+1))\n",
    "            new_content = ''\n",
    "            for j in range(start_index,i+1):\n",
    "                new_content += df['content'][j]\n",
    "            data.append({\n",
    "                        'file name': pdf,\n",
    "                        'content': new_content,\n",
    "                        'tokens': count_tokens(new_content)\n",
    "                        })\n",
    "            start_index = i+1\n",
    "new_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file name</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...</td>\n",
       "      <td>Water-soluble octahedral polyammonium nanocaps...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...</td>\n",
       "      <td>Rutgers, The State University of New Jersey, D...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...</td>\n",
       "      <td>Nanocapsule 5ewas prepared via the TFA-catalyz...</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...</td>\n",
       "      <td>/C2112009 Elsevier Ltd. All rights reserved.</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...</td>\n",
       "      <td>1. IntroductionMolecular container compounds a...</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29148</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...</td>\n",
       "      <td>67. Li, R.; Wu, Y. M.; Wang, C. H.; He, M.; Li...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29149</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...</td>\n",
       "      <td>One-Pot H/D Exchange and Low-Coordinated Iron ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29150</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...</td>\n",
       "      <td>68. Dwars, T.; Oehme, G. Complex-Catalyzed Hyd...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29151</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...</td>\n",
       "      <td>69. Limbach, H.-H.; Pery, T.; Rothermel, N.; C...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29152</th>\n",
       "      <td>E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...</td>\n",
       "      <td>2018 ,20, 10697 –10712.</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29153 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               file name  \\\n",
       "0      E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...   \n",
       "1      E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...   \n",
       "2      E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...   \n",
       "3      E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...   \n",
       "4      E:\\liuming\\cage-GPT\\papers\\no_SI\\2009----Water...   \n",
       "...                                                  ...   \n",
       "29148  E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...   \n",
       "29149  E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...   \n",
       "29150  E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...   \n",
       "29151  E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...   \n",
       "29152  E:\\liuming\\cage-GPT\\papers\\with_SI\\zhou-et-al-...   \n",
       "\n",
       "                                                 content  tokens  \n",
       "0      Water-soluble octahedral polyammonium nanocaps...      35  \n",
       "1      Rutgers, The State University of New Jersey, D...     134  \n",
       "2      Nanocapsule 5ewas prepared via the TFA-catalyz...     347  \n",
       "3           /C2112009 Elsevier Ltd. All rights reserved.      12  \n",
       "4      1. IntroductionMolecular container compounds a...     273  \n",
       "...                                                  ...     ...  \n",
       "29148  67. Li, R.; Wu, Y. M.; Wang, C. H.; He, M.; Li...      32  \n",
       "29149  One-Pot H/D Exchange and Low-Coordinated Iron ...      56  \n",
       "29150  68. Dwars, T.; Oehme, G. Complex-Catalyzed Hyd...      49  \n",
       "29151  69. Limbach, H.-H.; Pery, T.; Rothermel, N.; C...      93  \n",
       "29152                            2018 ,20, 10697 –10712.      12  \n",
       "\n",
       "[29153 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(data)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['new_content'] = ''\n",
    "new_df['part'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "part_num = 1\n",
    "txt = ''\n",
    "all_paper = {}\n",
    "for i in range(len(new_df)):\n",
    "    content = new_df.loc[i,'content']\n",
    "    filename1 = new_df.loc[i,'file name']\n",
    "    if i>0:\n",
    "        filename2 = new_df.loc[i-1,'file name']\n",
    "        if  filename1!=filename2  or i == len(new_df)-1:\n",
    "            index = 1\n",
    "            all_paper[filename2+'part'+str(part_num)] = txt\n",
    "            txt = ''\n",
    "            part_num = 1\n",
    "        if  token_num > 3300-count_tokens(content) or index >= 20:\n",
    "            index = 1\n",
    "            all_paper[filename1+'part'+str(part_num)] = txt\n",
    "            part_num += 1\n",
    "            txt = ''\n",
    "    txt += str(index)+'、'+content+'\\n \\n'\n",
    "    token_num = count_tokens(txt)\n",
    "    new_df.loc[i,'index'] = index\n",
    "    new_df.loc[i,'new_content'] = str(index)+'、'+content\n",
    "    new_df.loc[i,'part'] = filename1+'part'+str(part_num)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_excel(r\"E:\\liuming\\cage-GPT\\all_labelled_text.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "api_key = \"***\"\n",
    "organization = '***'\n",
    "client = OpenAI(api_key=api_key,organization=organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./classification.txt','r',encoding='utf-8') as f:\n",
    "    prompt = f.read()\n",
    "# with open('./paper_text.txt','r',encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "assistant = \"You are a classifier and you have learned the examples given in the system, \\\n",
    "now you are given a text containing many numbered paragraphs separated by a blank line, \\\n",
    "please output the category of each paragraph.Each category must be unambiguous, separated by a semicolon. \\\n",
    "If there is more than one category in a paragraph, output the main category.\\\n",
    "Write only the category in your answer, not the original text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_answers = {}\n",
    "data = []\n",
    "for index, key in enumerate(all_paper_list):\n",
    "    text = all_paper[key]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "                           model='gpt-4',\n",
    "                            messages=[      {\"role\": \"assistant\", \"content\": assistant},\n",
    "                                            {\"role\": \"system\", \"content\": prompt},\n",
    "                                            {\"role\": \"user\", \"content\": text}\n",
    "                                        ]\n",
    "                        )\n",
    "        answers = response.choices[0].message.content\n",
    "        all_answers[key] = answers\n",
    "        print(index+1,key)\n",
    "        data.append({\n",
    "                        'file name': key,\n",
    "                        'content': text,\n",
    "                        'categories': answers\n",
    "                        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "all_answers_keys = list(all_answers.keys())\n",
    "for index, key in enumerate(all_paper_list):\n",
    "    if key not in all_answers_keys:\n",
    "        text = all_paper[key]\n",
    "        data.append({'file name': key,'content': text})\n",
    "long_text_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, key in enumerate(all_paper.keys()):\n",
    "    if key in all_answers.keys():\n",
    "        text = all_paper[key].split('\\n \\n')\n",
    "        answers = re.split('\\n|;',all_answers[key])\n",
    "        answers = [answer for answer in answers if answer != '']\n",
    "        if len(text) > len(answers):\n",
    "            for i in range(len(answers)):\n",
    "                para = text[i]\n",
    "                answer = answers[i]\n",
    "                #answer = re.split('.|:',answer)[-1]\n",
    "                data.append({\n",
    "                            'file name': key,\n",
    "                            'content': para,\n",
    "                            'categories': answer\n",
    "                            })\n",
    "        else:\n",
    "            for i in range(len(text)):\n",
    "                para = text[i]\n",
    "                answer = answers[i]\n",
    "                #answer = re.split('.|:',answer)[-1]\n",
    "                data.append({\n",
    "                            'file name': key,\n",
    "                            'content': para,\n",
    "                            'categories': answer\n",
    "                            })\n",
    "class_df = pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
